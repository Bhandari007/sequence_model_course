{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEw9Xahn1XE3PdWFW/+GxZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhandari007/sequence_model_course/blob/main/w2a2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading starter files"
      ],
      "metadata": {
        "id": "H2IqKBHU_uWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a2/emo_utils.py\n",
        "# !wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a2/generateTestCases.py\n",
        "# !wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a2/test_utils.py\n",
        "# !wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a2/data/emojify_data.csv -P /data/\n",
        "# !wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a2/data/fake_glove.6B.50d.txt -P /data/\n",
        "\n",
        "# !wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a2/data/test_emoji.csv -P /data/\n",
        "# !wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a2/data/train_emoji.csv -P /data/\n",
        "!wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a2/data/tesss.csv -P /data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N_IOj_pAG9w",
        "outputId": "638e4804-0db6-4e22-dca2-459dccadbc3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-20 11:06:42--  https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a2/data/tesss.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1481 (1.4K) [text/plain]\n",
            "Saving to: ‘/data/tesss.csv’\n",
            "\n",
            "\rtesss.csv             0%[                    ]       0  --.-KB/s               \rtesss.csv           100%[===================>]   1.45K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-20 11:06:42 (15.8 MB/s) - ‘/data/tesss.csv’ saved [1481/1481]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"/data/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w00evUWsAwYN",
        "outputId": "a970df81-9c2b-4413-d1fc-167a19021caa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train_emoji.csv',\n",
              " 'emojify_data.csv',\n",
              " 'test_emoji.csv',\n",
              " 'fake_glove.6B.50d.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Emojify!\n",
        "\n",
        "In this notebook, we're going to use word vector representations to build an Emojifier.\n",
        "\n",
        "We'll implement a model which inputs a sequence and finds the most appropriate emoji to be used with that sentence.\n",
        "\n",
        "**Objectives:**\n",
        "\n",
        "* Create an embedding layer in Keras with pre-trained word vectors.\n",
        "* Explain the advantages and disadvantages of the GloVe algorithm.\n",
        "* Describe how negative sampling learns word vectors more efficiently than other methods.\n",
        "* Build a sentiment classifier using word embedding\n",
        "* Build and train a more sophisticated classifier using LSTM\n",
        "\n",
        "# Packages"
      ],
      "metadata": {
        "id": "4rhZdL1GBF7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2W8KIqeDVdT",
        "outputId": "42bf8bf9-e335-4bad-9898-7832353d7108"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.1.0.tar.gz (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 8.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.1.0-py3-none-any.whl size=212392 sha256=5e760902b736c07af388667360211258aa3af8fede30a250114950ff61076a49\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/75/99/51c2a119f4cfd3af7b49cc57e4f737bed7e40b348a85d82804\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from emo_utils import *\n",
        "import emoji\n",
        "import matplotlib.pyplot as plt\n",
        "from test_utils import *\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "PhKadQ2SDEit"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Baseline Model: Emojifier-V1\n",
        "\n",
        "# 1.1 - Dataset EMOJISET\n",
        "\n",
        "We will start by buildinga baseline classifier\n",
        "\n",
        "We have a tiny dataset (X,Y) where:\n",
        "  * X contains 127 sentences (strings).\n",
        "  * Y contains an integer label between 0 and 4 corresponding to an emoji for each sentence\n",
        "\n",
        "Load the dataset. The dataset is split between training (127 examples) and testing (56 examples).\n"
      ],
      "metadata": {
        "id": "GzDIkktTDSr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = read_csv(\"/data/train_emoji.csv\")\n",
        "X_test, Y_test = read_csv(\"/data/tesss.csv\")"
      ],
      "metadata": {
        "id": "o7zNtyxvEHFY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = len(max(X_train, key = len).split())\n",
        "maxlen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSiA6uMgE3KM",
        "outputId": "fd10679f-8627-425f-e3c4-c087f5d4ac1d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 - Overview of the Emojifier- V1\n",
        "\n",
        "In this section, we'll implement a baseline model called \"Emojifier-v1\".\n",
        "\n",
        "### Inputs and Outputs\n",
        "* The input of the model is a string corresponding to a sentence (e.g \"i love you\").\n",
        "* The output will be a probably vector of shape (1,5) (indicating that there are 5 emojis to choose from).\n",
        "* The (1,5) probability vector is passed to an argmax layer, which extracts the index of the emoji with the highest probability.\n",
        "\n",
        "### One-hot encoding\n",
        "* To get lables into a format suitable for training a softmax classifier, convert Y from its current shape (m,1) into a \"one-hot representation\" (m,5).\n",
        "  * Each row is a one-hot vector giving the label of one example.\n",
        "  * Here, `Y_oh` stands for \"Y-one-hot\" in the variable names `Y_oh_train` and `Y_oh_test`:"
      ],
      "metadata": {
        "id": "8pnc8UQnFcBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
        "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
      ],
      "metadata": {
        "id": "15Eu1h-dIUkB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NsJ4cAfYIW_e"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZS0rq1HIIZme"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
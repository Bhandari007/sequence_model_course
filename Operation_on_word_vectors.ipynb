{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaWTVSbOL7UY7ZYn1gNQDQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhandari007/sequence_model_course/blob/main/Operation_on_word_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading helper functions and data"
      ],
      "metadata": {
        "id": "Rcrr3pu6FOgm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjGrPnTmERe3",
        "outputId": "f7111776-2946-4ae7-e2ab-ec5709667b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-20 06:27:19--  https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a1/generateTestCases.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15806 (15K) [text/plain]\n",
            "Saving to: ‘generateTestCases.py’\n",
            "\n",
            "\rgenerateTestCases.p   0%[                    ]       0  --.-KB/s               \rgenerateTestCases.p 100%[===================>]  15.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-20 06:27:19 (31.5 MB/s) - ‘generateTestCases.py’ saved [15806/15806]\n",
            "\n",
            "--2022-10-20 06:27:19--  https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a1/w2v_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5037 (4.9K) [text/plain]\n",
            "Saving to: ‘w2v_utils.py’\n",
            "\n",
            "w2v_utils.py        100%[===================>]   4.92K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-20 06:27:19 (71.1 MB/s) - ‘w2v_utils.py’ saved [5037/5037]\n",
            "\n",
            "--2022-10-20 06:27:19--  https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a1/data/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13066 (13K) [text/plain]\n",
            "Saving to: ‘raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence Models/week2/w2a1/data/input.txt’\n",
            "\n",
            "raw.githubuserconte 100%[===================>]  12.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-20 06:27:19 (100 MB/s) - ‘raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence Models/week2/w2a1/data/input.txt’ saved [13066/13066]\n",
            "\n",
            "/data/: Scheme missing.\n",
            "FINISHED --2022-10-20 06:27:19--\n",
            "Total wall clock time: 0.1s\n",
            "Downloaded: 1 files, 13K in 0s (100 MB/s)\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a1/generateTestCases.py\n",
        "!wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a1/w2v_utils.py\n",
        "!wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week2/w2a1/data/input.txt -p /data/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://github.com/uclnlp/inferbeddings/blob/master/data/glove/glove.6B.50d.txt.gz?raw=true\n",
        "!gzip -d glove.6B.50d.txt.gz"
      ],
      "metadata": {
        "id": "VwaYv8fgJ-Jx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embeddings are very  computationally expensive to train, most ML practioners will load a pre-trained set of embeddings. In this notebook, we'll try our hand at loading, measuring similarity between and modifying pre-trained embeddings.\n",
        "\n",
        "**Objectives:**\n",
        "\n",
        "* Explain how word embeddings capture relationships between words\n",
        "* Load pre-trained word vectors\n",
        "* Measure similarity between word vectors using cosine similarity\n",
        "* Use embeddings to solve word analogy problems such as Man is to Woman as King is to __."
      ],
      "metadata": {
        "id": "JXSnmQ-YEuG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "lDNhaa5AFrek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from w2v_utils import *"
      ],
      "metadata": {
        "id": "ErlMiAl3GGIp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Load the Word Vectors\n",
        "\n",
        "For this notebook, we'll use 50-dimensional GloVe vectors to represent words."
      ],
      "metadata": {
        "id": "tzlT3g6HGJ_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
      ],
      "metadata": {
        "id": "vqVW6qEZGVJq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Embedding Vectors Versus One-Hot Vectors\n",
        "\n",
        "One-Hot Vectors don't do a good job of capturing the level of similarity between words. This is because every one-hot vector has the same Euclidean distance from any other one-hot vector.\n",
        "\n",
        "Embedding vectors, such as GloVe vectors, provide much more useful information about the meaning of individual words.<br>\n",
        "Now, see how we can use GloVe vectors to measure the similarity between two words!\n",
        "\n",
        "# 3 - Cosine Similarity\n",
        "\n",
        "To measure the similarity between two words, we need a way to measure the degrre of similarity between two embedding vectors for the two words. Given two vectors *u* and *v*, cosine similarity is defined as follows:\n",
        "\n",
        "CosineSimilarity(u,v) = (u.v) /( ||u||.||v||) = cos(theta)\n",
        "\n",
        "* The cosine similarity between on the angle between u and v.\n",
        "  * If u and v are very similar, their cosine similarity will be close to 1.\n",
        "  * If they are dissimilar, this cosine similarity will take a smaller value.\n",
        "\n",
        "\n",
        "### Exercise 1- cosine_similarity\n",
        "Implement the function `cosine_similarity()`"
      ],
      "metadata": {
        "id": "iy_qX4DoGc28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similarity between u and v\n",
        "        \n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)          \n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "    # Special case. Consider the case u = [0,0], v = [0,0]\n",
        "    if np.all(u == v):\n",
        "      return 1\n",
        "    \n",
        "    dot_product = np.dot(u,v)\n",
        "    (norm_1, norm_2) = (np.linalg.norm(u), np.linalg.norm(v))\n",
        "\n",
        "    cosine_similarity = dot_product / (norm_1 * norm_2)\n",
        "\n",
        "    # Avoid division by 0\n",
        "    if np.isclose(norm_1* norm_2,0,atol=1e-32):\n",
        "      return 0\n",
        "\n",
        "    return cosine_similarity\n"
      ],
      "metadata": {
        "id": "chNQ4LDaHNQ4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# START SKIP FOR GRADING\n",
        "father = word_to_vec_map[\"father\"]\n",
        "mother = word_to_vec_map[\"mother\"]\n",
        "ball = word_to_vec_map[\"ball\"]\n",
        "crocodile = word_to_vec_map[\"crocodile\"]\n",
        "france = word_to_vec_map[\"france\"]\n",
        "italy = word_to_vec_map[\"italy\"]\n",
        "paris = word_to_vec_map[\"paris\"]\n",
        "rome = word_to_vec_map[\"rome\"]\n",
        "\n",
        "print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n",
        "print(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\n",
        "print(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))\n",
        "# END SKIP FOR GRADING\n",
        "\n",
        "# PUBLIC TESTS\n",
        "def cosine_similarity_test(target):\n",
        "    a = np.random.uniform(-10, 10, 10)\n",
        "    b = np.random.uniform(-10, 10, 10)\n",
        "    c = np.random.uniform(-1, 1, 23)\n",
        "        \n",
        "    assert np.isclose(cosine_similarity(a, a), 1), \"cosine_similarity(a, a) must be 1\"\n",
        "    assert np.isclose(cosine_similarity((c >= 0) * 1, (c < 0) * 1), 0), \"cosine_similarity(a, not(a)) must be 0\"\n",
        "    assert np.isclose(cosine_similarity(a, -a), -1), \"cosine_similarity(a, -a) must be -1\"\n",
        "    assert np.isclose(cosine_similarity(a, b), cosine_similarity(a * 2, b * 4)), \"cosine_similarity must be scale-independent. You must divide by the product of the norms of each input\"\n",
        "\n",
        "    print(\"\\033[92mAll test passed!\")\n",
        "    \n",
        "cosine_similarity_test(cosine_similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQq9qNU9JLZH",
        "outputId": "a14db954-db36-4a86-b71a-7ef5faac9d5e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine_similarity(father, mother) =  0.8909038442893615\n",
            "cosine_similarity(ball, crocodile) =  0.2743924626137942\n",
            "cosine_similarity(france - paris, rome - italy) =  -0.6751479308174202\n",
            "\u001b[92mAll test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Word Analogy Task\n",
        "\n",
        "* In the word analogy task, complete this sentence:\n",
        "  \"*a* is to *b* as *c* is to **____**\".\n",
        "\n",
        "* An example is:\n",
        "  \"*man*\" is to \"*woman*\" as *king* is to *queen*\n",
        "\n",
        "* We're trying to find a word *d*, such that the associated word vectors *ea, eb, ec, ed* are related in the following manner:\n",
        "                eb - ea ~ ed - ec\n",
        "\n",
        "* Measure the similarity between eb-ea and ed-ec using cosine similarity.\n",
        "\n",
        "### Exercise 2 - complete analogy\n"
      ],
      "metadata": {
        "id": "lUUvLgJUJQHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
        "    \n",
        "    Arguments:\n",
        "    word_a -- a word, string\n",
        "    word_b -- a word, string\n",
        "    word_c -- a word, string\n",
        "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
        "    \n",
        "    Returns:\n",
        "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
        "    \"\"\"\n",
        "    \n",
        "    # convert to lowercase\n",
        "    word_a, word_b, word_c= (word_a.lower(), word_b.lower(), word_c.lower())\n",
        "\n",
        "    # get the word embeddings e_a, e_b, e_c\n",
        "    e_a, e_b , e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
        "\n",
        "    words = word_to_vec_map.keys()\n",
        "    max_cosine_sim = -100\n",
        "    best_word = None\n",
        "\n",
        "    for w in words:\n",
        "      if w == word_c:\n",
        "        continue\n",
        "      \n",
        "      cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n",
        "\n",
        "      if cosine_sim > max_cosine_sim:\n",
        "        max_cosine_sim = cosine_sim\n",
        "        best_word = w\n",
        "    \n",
        "    return best_word"
      ],
      "metadata": {
        "id": "fu7G0L2bMzHM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PUBLIC TEST\n",
        "def complete_analogy_test(target):\n",
        "    a = [3, 3] # Center at a\n",
        "    a_nw = [2, 4] # North-West oriented vector from a\n",
        "    a_s = [3, 2] # South oriented vector from a\n",
        "    \n",
        "    c = [-2, 1] # Center at c\n",
        "    # Create a controlled word to vec map\n",
        "    word_to_vec_map = {'a': a,\n",
        "                       'synonym_of_a': a,\n",
        "                       'a_nw': a_nw, \n",
        "                       'a_s': a_s, \n",
        "                       'c': c, \n",
        "                       'c_n': [-2, 2], # N\n",
        "                       'c_ne': [-1, 2], # NE\n",
        "                       'c_e': [-1, 1], # E\n",
        "                       'c_se': [-1, 0], # SE\n",
        "                       'c_s': [-2, 0], # S\n",
        "                       'c_sw': [-3, 0], # SW\n",
        "                       'c_w': [-3, 1], # W\n",
        "                       'c_nw': [-3, 2] # NW\n",
        "                      }\n",
        "    \n",
        "    # Convert lists to np.arrays\n",
        "    for key in word_to_vec_map.keys():\n",
        "        word_to_vec_map[key] = np.array(word_to_vec_map[key])\n",
        "            \n",
        "    assert(target('a', 'a_nw', 'c', word_to_vec_map) == 'c_nw')\n",
        "    assert(target('a', 'a_s', 'c', word_to_vec_map) == 'c_s')\n",
        "    assert(target('a', 'synonym_of_a', 'c', word_to_vec_map) != 'c'), \"Best word cannot be input query\"\n",
        "    assert(target('a', 'c', 'a', word_to_vec_map) == 'c')\n",
        "\n",
        "    print(\"\\033[92mAll tests passed\")\n",
        "    \n",
        "complete_analogy_test(complete_analogy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKPDEmwwOJV4",
        "outputId": "00b00aad-bd10-42f5-b54e-ac4a0e0de79d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mAll tests passed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# START SKIP FOR GRADING\n",
        "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
        "for triad in triads_to_try:\n",
        "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad, word_to_vec_map)))\n",
        "\n",
        "# END SKIP FOR GRADING"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwLEOM-QOMGS",
        "outputId": "14ff9b2c-1f57-4460-e48a-ef0987f8c6d7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "italy -> italian :: spain -> spanish\n",
            "india -> delhi :: japan -> tokyo\n",
            "man -> woman :: boy -> girl\n",
            "small -> smaller :: large -> smaller\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(complete_analogy('man','woman','father',word_to_vec_map))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w_z7QpNOOhY",
        "outputId": "35737689-6791-4577-9a4f-7d141dd479d0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mother\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MLUW_XHvOwih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
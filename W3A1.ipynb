{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiy9ymdYj96atewQA9nL2y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhandari007/sequence_model_course/blob/main/W3A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Starter Files"
      ],
      "metadata": {
        "id": "NkbRa3OfzvoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx9TJ2k7ykCg",
        "outputId": "fb236bd5-21cc-4c0c-cf03-6107bcdfc53d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-21 13:47:41--  https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week3/w3a1/generateTestCases.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2741 (2.7K) [text/plain]\n",
            "Saving to: ‘generateTestCases.py’\n",
            "\n",
            "\rgenerateTestCases.p   0%[                    ]       0  --.-KB/s               \rgenerateTestCases.p 100%[===================>]   2.68K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-21 13:47:41 (41.9 MB/s) - ‘generateTestCases.py’ saved [2741/2741]\n",
            "\n",
            "--2022-10-21 13:47:42--  https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week3/w3a1/nmt_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8756 (8.6K) [text/plain]\n",
            "Saving to: ‘nmt_utils.py’\n",
            "\n",
            "nmt_utils.py        100%[===================>]   8.55K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-21 13:47:42 (60.3 MB/s) - ‘nmt_utils.py’ saved [8756/8756]\n",
            "\n",
            "--2022-10-21 13:47:42--  https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week3/w3a1/test_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2307 (2.3K) [text/plain]\n",
            "Saving to: ‘test_utils.py’\n",
            "\n",
            "test_utils.py       100%[===================>]   2.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-21 13:47:42 (35.5 MB/s) - ‘test_utils.py’ saved [2307/2307]\n",
            "\n",
            "--2022-10-21 13:47:42--  https://github.com/abdur75648/Deep-Learning-Specialization-Coursera/blob/main/Sequence%20Models/week3/w3a1/models/model.h5?raw=true\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/abdur75648/Deep-Learning-Specialization-Coursera/raw/main/Sequence%20Models/week3/w3a1/models/model.h5 [following]\n",
            "--2022-10-21 13:47:42--  https://github.com/abdur75648/Deep-Learning-Specialization-Coursera/raw/main/Sequence%20Models/week3/w3a1/models/model.h5\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week3/w3a1/models/model.h5 [following]\n",
            "--2022-10-21 13:47:42--  https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week3/w3a1/models/model.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 698400 (682K) [application/octet-stream]\n",
            "Saving to: ‘/models/model.h5?raw=true’\n",
            "\n",
            "model.h5?raw=true   100%[===================>] 682.03K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-10-21 13:47:43 (15.6 MB/s) - ‘/models/model.h5?raw=true’ saved [698400/698400]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week3/w3a1/generateTestCases.py\n",
        "!wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week3/w3a1/nmt_utils.py\n",
        "!wget https://raw.githubusercontent.com/abdur75648/Deep-Learning-Specialization-Coursera/main/Sequence%20Models/week3/w3a1/test_utils.py\n",
        "!wget https://github.com/abdur75648/Deep-Learning-Specialization-Coursera/blob/main/Sequence%20Models/week3/w3a1/models/model.h5?raw=true -P /models/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"/models\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsp6TqGGzEsF",
        "outputId": "e5786efd-8c6b-4207-d0e6-56ec7205b1b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.h5?raw=true']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Machine Translation\n",
        "\n",
        "**Objectives**\n",
        "\n",
        "* To build a Neural Machine Translation (NMT) model to translate human-readable dates (\"25th of June, 2009\") into machine-readable dates (\"2009-06-25\").\n",
        "* To build do this using an attention model, one of the most sophisitcated sequence-sequence models."
      ],
      "metadata": {
        "id": "ik1p5xRzzQQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "Qo948msc6CGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekOWZsIq58Z-",
        "outputId": "ce9b72e3-7bd2-4083-a9f1-76567c5ea969"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faker\n",
            "  Downloading Faker-15.1.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0.1 in /usr/local/lib/python3.7/dist-packages (from faker) (4.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.7/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.4->faker) (1.15.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-15.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "G_65I6Wl50PR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Translating Human Readable Dates into Machine Readable Dates\n",
        "\n",
        "* The model we will build here could be used to translate from one language to another, sych as translating from English to Nepali.\n",
        "\n",
        "* However, language translation requires massive datasets and usually takes days of training on GPUs.\n",
        "\n",
        "* To give us a place to experiment with these models without using massive datasets, we will perform a simpler \"date translation\" task.\n"
      ],
      "metadata": {
        "id": "emrx_w5g55-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 - Dataset\n",
        "\n",
        "We will train the model on a dataset of 10,000 human readable dates and their equivalent, standardized, machine readable dates."
      ],
      "metadata": {
        "id": "fYp8xGzD6oB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = 10000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4dyCy7S6qVg",
        "outputId": "309f78ab-b314-4422-cdd1-0a8f78794b14"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 17060.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZrQDWR864H_",
        "outputId": "b883bd59-3ad3-450c-b684-b8cb36e57168"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('9 may 1998', '1998-05-09'),\n",
              " ('10.11.19', '2019-11-10'),\n",
              " ('9/10/70', '1970-09-10'),\n",
              " ('saturday april 28 1990', '1990-04-28'),\n",
              " ('thursday january 26 1995', '1995-01-26'),\n",
              " ('monday march 7 1983', '1983-03-07'),\n",
              " ('sunday may 22 1988', '1988-05-22'),\n",
              " ('08 jul 2008', '2008-07-08'),\n",
              " ('8 sep 1999', '1999-09-08'),\n",
              " ('thursday january 1 1981', '1981-01-01')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've loaded:\n",
        "\n",
        "* dataset: a list of tuples (human readable date, machine readable date).\n",
        "* human_vocab: a python dictionary mapping all characters used in the human readable dates to an integer-valued index\n",
        "* machine_vocab: a python dictionary mapping all characters used in machine readbale dates to an integer-valued index\n",
        "* inv_machine_vocab: the inverse dictionary of machine_vocab, mapping from indices back to characters."
      ],
      "metadata": {
        "id": "oznWdqNr66hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1ZgOwQh7daf",
        "outputId": "0309375b-fd55-4c4b-e8f5-f8617a025575"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " '.': 1,\n",
              " '/': 2,\n",
              " '0': 3,\n",
              " '1': 4,\n",
              " '2': 5,\n",
              " '3': 6,\n",
              " '4': 7,\n",
              " '5': 8,\n",
              " '6': 9,\n",
              " '7': 10,\n",
              " '8': 11,\n",
              " '9': 12,\n",
              " 'a': 13,\n",
              " 'b': 14,\n",
              " 'c': 15,\n",
              " 'd': 16,\n",
              " 'e': 17,\n",
              " 'f': 18,\n",
              " 'g': 19,\n",
              " 'h': 20,\n",
              " 'i': 21,\n",
              " 'j': 22,\n",
              " 'l': 23,\n",
              " 'm': 24,\n",
              " 'n': 25,\n",
              " 'o': 26,\n",
              " 'p': 27,\n",
              " 'r': 28,\n",
              " 's': 29,\n",
              " 't': 30,\n",
              " 'u': 31,\n",
              " 'v': 32,\n",
              " 'w': 33,\n",
              " 'y': 34,\n",
              " '<unk>': 35,\n",
              " '<pad>': 36}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"Xoh.shape:\", Xoh.shape)\n",
        "print(\"Yoh.shape:\", Yoh.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHbTNx877g5_",
        "outputId": "784a485c-11c8-45c0-dd0a-3d3bedef738d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape: (10000, 30)\n",
            "Y.shape: (10000, 10)\n",
            "Xoh.shape: (10000, 30, 37)\n",
            "Yoh.shape: (10000, 10, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have:\n",
        "* X: a processed version of the human readable dates in the training set.\n",
        "  * Each character in X is replaced by an index (integer) mapped to the character using `human_vocab`.\n",
        "  * Each dates is padded to ensure a length of Tx using a special character (< pad >).\n",
        "  * X.shape = (m, Tx) where m is the number of training examples in a batch\n",
        "\n",
        "* Y: a processed verions of the machine readable dates in the training set.\n",
        "  * Each character is replaced by the index it is mapped to in machine_vocab.\n",
        "  * Y.shape = (m, Ty).\n",
        "\n",
        "* Xoh: one-hot version of X.\n",
        "  * Each index in X is converted to the one-hot representation.\n",
        "  * Xoh.shape = (m, Tx, len(human_vocab))\n",
        "\n",
        "* Yoh: one-hot version of Y\n",
        "  * Each index in Y is converted to the one-hot representation.\n",
        "  * Yon.shape = (m, ty, len(machine_vocab))\n",
        "  * len(machine_vocab) = 11, since there are 10 numeric digits and the - symbol"
      ],
      "metadata": {
        "id": "OJt-ytQH7lIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "print(\"Source date:\", dataset[index][0])\n",
        "print(\"Target date:\", dataset[index][1])\n",
        "print()\n",
        "print(\"Source after preprocessing (indices):\", X[index])\n",
        "print(\"Target after preprocessing (indices):\", Y[index])\n",
        "print()\n",
        "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
        "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q377FpoG8yav",
        "outputId": "57c00852-379c-4dea-992c-3e5dea35cb08"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source date: 9 may 1998\n",
            "Target date: 1998-05-09\n",
            "\n",
            "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
            " 36 36 36 36 36 36]\n",
            "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
            "\n",
            "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Neural Machine Translation with Attention\n",
        "\n",
        "* If we had to translate a book's paragraph from French to English, we would not read the whole paragraph, then close the book and translate.\n",
        "* Even during the translation process, we would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English we are writing down.\n",
        "* The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step.\n"
      ],
      "metadata": {
        "id": "H8F1fkmp8z8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined shared layers as global variables\n",
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = \"tanh\")\n",
        "densor2 = Dense(1, activation = \"relu\")\n",
        "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)"
      ],
      "metadata": {
        "id": "K4PbAek19ZMl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-step attention"
      ],
      "metadata": {
        "id": "gNNv-dnC-GL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def one_step_attention(a, s_prev):\n",
        "    \"\"\"\n",
        "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
        "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
        "    \n",
        "    Arguments:\n",
        "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
        "    \n",
        "    Returns:\n",
        "    context -- context vector, input of the next (post-attention) LSTM cell\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
        "    s_prev = repeator(s_prev)\n",
        "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
        "    # For grading purposes, please list 'a' first and 's_prev' second, in this order.\n",
        "    concat = concatenator([a,s_prev])\n",
        "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
        "    e = densor1(concat)\n",
        "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
        "    energies = densor2(e)\n",
        "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
        "    alphas = activator(energies)\n",
        "    # Use dotor together with \"alphas\" and \"a\", in this order, to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
        "    context = dotor([alphas,a])\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return context"
      ],
      "metadata": {
        "id": "HL8n6Rsc-PGl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "def one_step_attention_test(target):\n",
        "\n",
        "    m = 10\n",
        "    Tx = 30\n",
        "    n_a = 32\n",
        "    n_s = 64\n",
        "    #np.random.seed(10)\n",
        "    a = np.random.uniform(1, 0, (m, Tx, 2 * n_a)).astype(np.float32)\n",
        "    s_prev =np.random.uniform(1, 0, (m, n_s)).astype(np.float32) * 1\n",
        "    context = target(a, s_prev)\n",
        "    \n",
        "    # assert type(context) == tf.Tensor, \"Unexpected type. It should be a Tensor\"\n",
        "    assert tuple(context.shape) == (m, 1, n_s), \"Unexpected output shape\"\n",
        "    assert np.all(context.numpy() > 0), \"All output values must be > 0 in this example\"\n",
        "    assert np.all(context.numpy() < 1), \"All output values must be < 1 in this example\"\n",
        "\n",
        "    #assert np.allclose(context[0][0][0:5].numpy(), [0.50877404, 0.57160693, 0.45448175, 0.50074816, 0.53651875]), \"Unexpected values in the result\"\n",
        "    print(\"\\033[92mAll tests passed!\")\n",
        "    \n",
        "one_step_attention_test(one_step_attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBRzOu9q-PmW",
        "outputId": "3959f83b-df57-41c8-c09d-d1e2a13a3907"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mAll tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### modelf"
      ],
      "metadata": {
        "id": "IhSHod9r-Shd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
        "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
        "\n",
        "# Please note, this is the post attention LSTM cell.  \n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # Please do not modify this global variable.\n",
        "output_layer = Dense(len(machine_vocab), activation=softmax)"
      ],
      "metadata": {
        "id": "ohm8maan-nLe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modelf(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_a -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
        "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the inputs of your model with a shape (Tx,)\n",
        "    # Define s0 (initial hidden state) and c0 (initial cell state)\n",
        "    # for the decoder LSTM with shape (n_s,)\n",
        "    X = Input(shape=(Tx, human_vocab_size))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    \n",
        "    # Initialize empty list of outputs\n",
        "    outputs = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Step 1: Define your pre-attention Bi-LSTM. (≈ 1 line)\n",
        "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
        "\n",
        "    # Step 2: Iterate for Ty steps\n",
        "    for t in range(Ty):\n",
        "    \n",
        "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
        "        context = one_step_attention(a, s)\n",
        "        \n",
        "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "        s, _, c = post_activation_LSTM_cell(context,initial_state=[s, c])\n",
        "        \n",
        "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
        "        out = output_layer(s)\n",
        "        \n",
        "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
        "        outputs.append(out)\n",
        "    \n",
        "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
        "    model = Model(inputs=[X, s0, c0],outputs=outputs)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "1nRd7Qo9-oxR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "from test_utils import *\n",
        "\n",
        "def modelf_test(target):\n",
        "    m = 10\n",
        "    Tx = 30\n",
        "    n_a = 32\n",
        "    n_s = 64\n",
        "    len_human_vocab = 37\n",
        "    len_machine_vocab = 11\n",
        "    \n",
        "    \n",
        "    model = target(Tx, Ty, n_a, n_s, len_human_vocab, len_machine_vocab)\n",
        "    \n",
        "    print(summary(model))\n",
        "\n",
        "    \n",
        "    expected_summary = [['InputLayer', [(None, 30, 37)], 0],\n",
        "                         ['InputLayer', [(None, 64)], 0],\n",
        "                         ['Bidirectional', (None, 30, 64), 17920],\n",
        "                         ['RepeatVector', (None, 30, 64), 0, 30],\n",
        "                         ['Concatenate', (None, 30, 128), 0],\n",
        "                         ['Dense', (None, 30, 10), 1290, 'tanh'],\n",
        "                         ['Dense', (None, 30, 1), 11, 'relu'],\n",
        "                         ['Activation', (None, 30, 1), 0],\n",
        "                         ['Dot', (None, 1, 64), 0],\n",
        "                         ['InputLayer', [(None, 64)], 0],\n",
        "                         ['LSTM',[(None, 64), (None, 64), (None, 64)], 33024,[(None, 1, 64), (None, 64), (None, 64)],'tanh'],\n",
        "                         ['Dense', (None, 11), 715, 'softmax']]\n",
        "\n",
        "    comparator(summary(model), expected_summary)\n",
        "    \n",
        "\n",
        "modelf_test(modelf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBTv2bVF-yJ8",
        "outputId": "ad911890-eff7-4cf2-f437-96870f014f37"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['InputLayer', [(None, 30, 37)], 0], ['InputLayer', [(None, 64)], 0], ['Bidirectional', (None, 30, 64), 17920], ['RepeatVector', (None, 30, 64), 0, 30], ['Concatenate', (None, 30, 128), 0], ['Dense', (None, 30, 10), 1290, 'tanh'], ['Dense', (None, 30, 1), 11, 'relu'], ['Activation', (None, 30, 1), 0], ['Dot', (None, 1, 64), 0], ['InputLayer', [(None, 64)], 0], ['LSTM', [(None, 64), (None, 64), (None, 64)], 33024, [(None, 1, 64), (None, 64), (None, 64)], 'tanh'], ['Dense', (None, 11), 715, 'softmax']]\n",
            "\u001b[32mAll tests passed!\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = modelf(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
      ],
      "metadata": {
        "id": "focANm25-1oQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63LoKp_B-4ru",
        "outputId": "c16c806a-4f5b-46d9-b850-9ceffc6807e9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 30, 37)]     0           []                               \n",
            "                                                                                                  \n",
            " s0 (InputLayer)                [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 30, 64)      17920       ['input_2[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " repeat_vector (RepeatVector)   (None, 30, 64)       0           ['s0[0][0]',                     \n",
            "                                                                  'lstm[10][0]',                  \n",
            "                                                                  'lstm[11][0]',                  \n",
            "                                                                  'lstm[12][0]',                  \n",
            "                                                                  'lstm[13][0]',                  \n",
            "                                                                  'lstm[14][0]',                  \n",
            "                                                                  'lstm[15][0]',                  \n",
            "                                                                  'lstm[16][0]',                  \n",
            "                                                                  'lstm[17][0]',                  \n",
            "                                                                  'lstm[18][0]']                  \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 30, 128)      0           ['bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[10][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[11][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[12][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[13][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[14][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[15][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[16][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[17][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[18][0]',         \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'repeat_vector[19][0]']         \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 30, 10)       1290        ['concatenate[10][0]',           \n",
            "                                                                  'concatenate[11][0]',           \n",
            "                                                                  'concatenate[12][0]',           \n",
            "                                                                  'concatenate[13][0]',           \n",
            "                                                                  'concatenate[14][0]',           \n",
            "                                                                  'concatenate[15][0]',           \n",
            "                                                                  'concatenate[16][0]',           \n",
            "                                                                  'concatenate[17][0]',           \n",
            "                                                                  'concatenate[18][0]',           \n",
            "                                                                  'concatenate[19][0]']           \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 30, 1)        11          ['dense[10][0]',                 \n",
            "                                                                  'dense[11][0]',                 \n",
            "                                                                  'dense[12][0]',                 \n",
            "                                                                  'dense[13][0]',                 \n",
            "                                                                  'dense[14][0]',                 \n",
            "                                                                  'dense[15][0]',                 \n",
            "                                                                  'dense[16][0]',                 \n",
            "                                                                  'dense[17][0]',                 \n",
            "                                                                  'dense[18][0]',                 \n",
            "                                                                  'dense[19][0]']                 \n",
            "                                                                                                  \n",
            " attention_weights (Activation)  (None, 30, 1)       0           ['dense_1[10][0]',               \n",
            "                                                                  'dense_1[11][0]',               \n",
            "                                                                  'dense_1[12][0]',               \n",
            "                                                                  'dense_1[13][0]',               \n",
            "                                                                  'dense_1[14][0]',               \n",
            "                                                                  'dense_1[15][0]',               \n",
            "                                                                  'dense_1[16][0]',               \n",
            "                                                                  'dense_1[17][0]',               \n",
            "                                                                  'dense_1[18][0]',               \n",
            "                                                                  'dense_1[19][0]']               \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 1, 64)        0           ['attention_weights[10][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[11][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[12][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[13][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[14][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[15][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[16][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[17][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[18][0]',     \n",
            "                                                                  'bidirectional_1[0][0]',        \n",
            "                                                                  'attention_weights[19][0]',     \n",
            "                                                                  'bidirectional_1[0][0]']        \n",
            "                                                                                                  \n",
            " c0 (InputLayer)                [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 64),         33024       ['dot[10][0]',                   \n",
            "                                 (None, 64),                      's0[0][0]',                     \n",
            "                                 (None, 64)]                      'c0[0][0]',                     \n",
            "                                                                  'dot[11][0]',                   \n",
            "                                                                  'lstm[10][0]',                  \n",
            "                                                                  'lstm[10][2]',                  \n",
            "                                                                  'dot[12][0]',                   \n",
            "                                                                  'lstm[11][0]',                  \n",
            "                                                                  'lstm[11][2]',                  \n",
            "                                                                  'dot[13][0]',                   \n",
            "                                                                  'lstm[12][0]',                  \n",
            "                                                                  'lstm[12][2]',                  \n",
            "                                                                  'dot[14][0]',                   \n",
            "                                                                  'lstm[13][0]',                  \n",
            "                                                                  'lstm[13][2]',                  \n",
            "                                                                  'dot[15][0]',                   \n",
            "                                                                  'lstm[14][0]',                  \n",
            "                                                                  'lstm[14][2]',                  \n",
            "                                                                  'dot[16][0]',                   \n",
            "                                                                  'lstm[15][0]',                  \n",
            "                                                                  'lstm[15][2]',                  \n",
            "                                                                  'dot[17][0]',                   \n",
            "                                                                  'lstm[16][0]',                  \n",
            "                                                                  'lstm[16][2]',                  \n",
            "                                                                  'dot[18][0]',                   \n",
            "                                                                  'lstm[17][0]',                  \n",
            "                                                                  'lstm[17][2]',                  \n",
            "                                                                  'dot[19][0]',                   \n",
            "                                                                  'lstm[18][0]',                  \n",
            "                                                                  'lstm[18][2]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 11)           715         ['lstm[10][0]',                  \n",
            "                                                                  'lstm[11][0]',                  \n",
            "                                                                  'lstm[12][0]',                  \n",
            "                                                                  'lstm[13][0]',                  \n",
            "                                                                  'lstm[14][0]',                  \n",
            "                                                                  'lstm[15][0]',                  \n",
            "                                                                  'lstm[16][0]',                  \n",
            "                                                                  'lstm[17][0]',                  \n",
            "                                                                  'lstm[18][0]',                  \n",
            "                                                                  'lstm[19][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 52,960\n",
            "Trainable params: 52,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile the model"
      ],
      "metadata": {
        "id": "bYgATpsA-8HF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ### (≈2 lines)\n",
        "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SM3ZaWX_Chr",
        "outputId": "a1b64433-aaac-40fe-a6ce-aec13c5e3443"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TESTS\n",
        "assert opt.lr == 0.005, \"Set the lr parameter to 0.005\"\n",
        "assert opt.beta_1 == 0.9, \"Set the beta_1 parameter to 0.9\"\n",
        "assert opt.beta_2 == 0.999, \"Set the beta_2 parameter to 0.999\"\n",
        "assert opt.decay == 0.01, \"Set the decay parameter to 0.01\"\n",
        "assert model.loss == \"categorical_crossentropy\", \"Wrong loss. Use 'categorical_crossentropy'\"\n",
        "assert model.optimizer == opt, \"Use the optimizer that you have instantiated\"\n",
        "assert model.compiled_metrics._user_metrics[0] == 'accuracy', \"set metrics to ['accuracy']\"\n",
        "\n",
        "print(\"\\033[92mAll tests passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDeRMOZJ_D1H",
        "outputId": "f8510480-122a-4faf-88ad-288d4a982121"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mAll tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define inputs and outputs, and fit the model"
      ],
      "metadata": {
        "id": "RwYF9uTR_GOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "outputs = list(Yoh.swapaxes(0,1))"
      ],
      "metadata": {
        "id": "go2fwKaX_Lzb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeuGc4Pl_NCx",
        "outputId": "ed7c3639-fe3c-4d79-c863-4619f62e6cec"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 31s 97ms/step - loss: 16.6526 - dense_2_loss: 1.2508 - dense_2_1_loss: 1.0745 - dense_2_2_loss: 1.8304 - dense_2_3_loss: 2.6625 - dense_2_4_loss: 0.7635 - dense_2_5_loss: 1.2766 - dense_2_6_loss: 2.7061 - dense_2_7_loss: 0.9158 - dense_2_8_loss: 1.6182 - dense_2_9_loss: 2.5540 - dense_2_accuracy: 0.4913 - dense_2_1_accuracy: 0.6554 - dense_2_2_accuracy: 0.2740 - dense_2_3_accuracy: 0.0933 - dense_2_4_accuracy: 0.9497 - dense_2_5_accuracy: 0.3306 - dense_2_6_accuracy: 0.0603 - dense_2_7_accuracy: 0.8816 - dense_2_8_accuracy: 0.3366 - dense_2_9_accuracy: 0.1196\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7d5fbf48d0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/models/model.h5?raw=true')"
      ],
      "metadata": {
        "id": "8NCfk_Hh_OtB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
        "s00 = np.zeros((1, n_s))\n",
        "c00 = np.zeros((1, n_s))\n",
        "for example in EXAMPLES:\n",
        "    source = string_to_int(example, Tx, human_vocab)\n",
        "    #print(source)\n",
        "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
        "    source = np.swapaxes(source, 0, 1)\n",
        "    source = np.expand_dims(source, axis=0)\n",
        "    prediction = model.predict([source, s00, c00])\n",
        "    prediction = np.argmax(prediction, axis = -1)\n",
        "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "    print(\"source:\", example)\n",
        "    print(\"output:\", ''.join(output),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDGiwpc6_hDv",
        "outputId": "44f19ee2-0a7c-4db1-e6fd-97def8ce59b2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 6s 6s/step\n",
            "source: 3 May 1979\n",
            "output: 1979-05-33 \n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "source: 5 April 09\n",
            "output: 2009-04-05 \n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "source: 21th of August 2016\n",
            "output: 2016-08-20 \n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "source: Tue 10 Jul 2007\n",
            "output: 2007-07-10 \n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "source: Saturday May 9 2018\n",
            "output: 2018-05-09 \n",
            "\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "source: March 3 2001\n",
            "output: 2001-03-03 \n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "source: March 3rd 2001\n",
            "output: 2001-03-03 \n",
            "\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "source: 1 March 2001\n",
            "output: 2001-03-01 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_date(sentence):\n",
        "    source = string_to_int(sentence, Tx, human_vocab)\n",
        "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
        "    source = np.swapaxes(source, 0, 1)\n",
        "    source = np.expand_dims(source, axis=0)\n",
        "    prediction = model.predict([source, s00, c00])\n",
        "    prediction = np.argmax(prediction, axis = -1)\n",
        "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "    print(\"source:\", sentence)\n",
        "    print(\"output:\", ''.join(output),\"\\n\")\n",
        "example = \"4th of july 2001\"\n",
        "translate_date(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "375aZ4l4_qUp",
        "outputId": "9b4556cb-ff43-4200-9650-b94e3aff6fb9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n",
            "source: 4th of july 2001\n",
            "output: 2001-07-04 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ThK8sStn_t-F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}